# download_nllb_model.py
# NLLB ëª¨ë¸ì„ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (ê°œì„ ëœ ë²„ì „)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from pathlib import Path
import os
import sys
from tqdm import tqdm
import torch
import shutil

# ì‚¬ìš© ê°€ëŠ¥í•œ NLLB ëª¨ë¸ ëª©ë¡
AVAILABLE_MODELS = {
    "600M": {
        "name": "facebook/nllb-200-distilled-600M",
        "size": "2.4GB",
        "description": "ê²½ëŸ‰í™”ëœ ëª¨ë¸ (ë¹ ë¦„, í’ˆì§ˆ ë³´í†µ)",
        "local_dir": "./models/nllb-200-distilled-600M"
    },
    "1.3B": {
        "name": "facebook/nllb-200-1.3B", 
        "size": "5.2GB",
        "description": "ì¤‘ê°„ í¬ê¸° ëª¨ë¸ (ê· í˜•ì¡íŒ ì„±ëŠ¥)",
        "local_dir": "./models/nllb-200-1.3B"
    },
    "3.3B": {
        "name": "facebook/nllb-200-3.3B",
        "size": "13GB", 
        "description": "ëŒ€ìš©ëŸ‰ ëª¨ë¸ (ëŠë¦¼, ìµœê³  í’ˆì§ˆ)",
        "local_dir": "./models/nllb-200-3.3B"
    }
}

def show_model_menu():
    """ëª¨ë¸ ì„ íƒ ë©”ë‰´ í‘œì‹œ"""
    print("ğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ NLLB ëª¨ë¸:")
    print("-" * 80)
    
    for key, info in AVAILABLE_MODELS.items():
        print(f"{key:>4} | {info['size']:>6} | {info['description']}")
        print(f"     | ëª¨ë¸ëª…: {info['name']}")
        print(f"     | ì €ì¥ìœ„ì¹˜: {info['local_dir']}")
        print("-" * 80)
    
    print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    print("   - ë²ˆì—­ í’ˆì§ˆ ìš°ì„ : 1.3B ì„ íƒ")
    print("   - ì†ë„ ìš°ì„ : 600M ì„ íƒ") 
    print("   - ìµœê³  í’ˆì§ˆ (GPU í•„ìš”): 3.3B ì„ íƒ")
    print()

def get_user_choice():
    """ì‚¬ìš©ì ì„ íƒ ë°›ê¸°"""
    while True:
        choice = input("ì–´ë–¤ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (600M/1.3B/3.3B): ").strip().upper()
        if choice in AVAILABLE_MODELS:
            return choice
        else:
            print("âŒ ì˜¬ë°”ë¥¸ ì„ íƒì´ ì•„ë‹™ë‹ˆë‹¤. 600M, 1.3B, 3.3B ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

def check_disk_space(required_gb):
    """ë””ìŠ¤í¬ ê³µê°„ í™•ì¸"""
    try:
        current_dir = Path.cwd()
        free_space = shutil.disk_usage(current_dir).free
        free_gb = free_space / (1024**3)
        
        print(f"ğŸ’¾ ë””ìŠ¤í¬ ì—¬ìœ  ê³µê°„: {free_gb:.1f}GB")
        print(f"ğŸ“¦ í•„ìš”í•œ ê³µê°„: ~{required_gb}GB")
        
        if free_gb < required_gb * 1.2:  # 20% ì—¬ìœ  ê³µê°„ ê³ ë ¤
            print(f"âš ï¸  ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
            choice = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            return choice in ['y', 'yes']
        else:
            print("âœ… ë””ìŠ¤í¬ ê³µê°„ ì¶©ë¶„")
            return True
            
    except Exception as e:
        print(f"âš ï¸  ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # í™•ì¸ ì‹¤íŒ¨ ì‹œì—ë„ ì§„í–‰

def download_nllb_model(model_key):
    """ì„ íƒëœ NLLB ëª¨ë¸ì„ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œ"""
    
    model_info = AVAILABLE_MODELS[model_key]
    model_name = model_info["name"]
    local_dir = model_info["local_dir"]
    size_str = model_info["size"]
    
    print(f"ğŸš€ NLLB ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_name}")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {local_dir}")
    print(f"ğŸ“Š ì˜ˆìƒ í¬ê¸°: {size_str}")
    print(f"ğŸ”§ ì„¤ëª…: {model_info['description']}")
    print("-" * 70)
    
    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
    required_gb = float(size_str.replace('GB', ''))
    if not check_disk_space(required_gb):
        print("âŒ ë‹¤ìš´ë¡œë“œ ì¤‘ë‹¨ë¨")
        return None
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    Path(local_dir).mkdir(parents=True, exist_ok=True)
    
    try:
        # 1. í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
        print("1ï¸âƒ£ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=None,  # ìºì‹œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            local_files_only=False,
            force_download=False,
            resume_download=True  # ì¤‘ë‹¨ëœ ë‹¤ìš´ë¡œë“œ ì¬ê°œ
        )
        print("âœ… í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        
        # 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì§„í–‰ë¥  í‘œì‹œ)
        print("2ï¸âƒ£ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        print("   ğŸ’¡ íŒ: ì¸í„°ë„· ì—°ê²°ì´ ì¤‘ë‹¨ë˜ë©´ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”. ì´ì–´ì„œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.")
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë‹¤ìš´ë¡œë“œ
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            cache_dir=None,
            local_files_only=False,
            force_download=False,
            resume_download=True,
            torch_dtype=torch.float32,  # í˜¸í™˜ì„±ì„ ìœ„í•´ float32 ì‚¬ìš©
            low_cpu_mem_usage=True  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
        )
        print("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        
        # 3. ë¡œì»¬ì— ì €ì¥
        print("3ï¸âƒ£ ë¡œì»¬ì— ì €ì¥ ì¤‘...")
        tokenizer.save_pretrained(local_dir)
        model.save_pretrained(local_dir)
        print("âœ… ë¡œì»¬ ì €ì¥ ì™„ë£Œ!")
        
        # 4. ë‹¤ìš´ë¡œë“œ í™•ì¸
        print("4ï¸âƒ£ ë‹¤ìš´ë¡œë“œ íŒŒì¼ í™•ì¸...")
        verify_download(local_dir)
        
        print("-" * 70)
        print("ğŸ‰ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸ“ ëª¨ë¸ ìœ„ì¹˜: {os.path.abspath(local_dir)}")
        
        return local_dir
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        print("ğŸ’¡ ë‹¤ìŒì— ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("\nğŸ”§ ë¬¸ì œ í•´ê²° ë°©ë²•:")
        print("1. ì¸í„°ë„· ì—°ê²° í™•ì¸")
        print("2. ë””ìŠ¤í¬ ê³µê°„ í™•ì¸") 
        print("3. Python íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸: pip install --upgrade transformers torch")
        print("4. í”„ë¡ì‹œ ì„¤ì • í™•ì¸")
        return None

def verify_download(model_path):
    """ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ í™•ì¸"""
    model_dir = Path(model_path)
    
    # í•„ìˆ˜ íŒŒì¼ í™•ì¸
    required_files = [
        "config.json",
        "tokenizer.json", 
        "tokenizer_config.json",
        "special_tokens_map.json"
    ]
    
    missing_files = []
    for filename in required_files:
        filepath = model_dir / filename
        if filepath.exists():
            print(f"   âœ… {filename}")
        else:
            print(f"   âŒ {filename} (ëˆ„ë½)")
            missing_files.append(filename)
    
    # ëª¨ë¸ íŒŒì¼ í™•ì¸
    model_files = list(model_dir.glob("pytorch_model*.bin")) + \
                 list(model_dir.glob("model*.safetensors"))
    
    total_size_mb = 0
    if model_files:
        for model_file in model_files:
            size_mb = model_file.stat().st_size / (1024 * 1024)
            total_size_mb += size_mb
            print(f"   âœ… {model_file.name} ({size_mb:.1f}MB)")
    else:
        print("   âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        missing_files.append("model files")
    
    if total_size_mb > 0:
        print(f"   ğŸ“Š ì´ ëª¨ë¸ í¬ê¸°: {total_size_mb:.1f}MB")
    
    if missing_files:
        print(f"   âš ï¸  ëˆ„ë½ëœ íŒŒì¼: {', '.join(missing_files)}")
        return False
    else:
        print("   âœ… ëª¨ë“  íŒŒì¼ í™•ì¸ë¨")
        return True

def test_model(model_path):
    """ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    try:
        from transformers import pipeline
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        print("   ğŸ“ íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")
        translator = pipeline(
            "translation",
            model=model_path,
            tokenizer=model_path,
            device=-1,  # CPU ì‚¬ìš©
            torch_dtype=torch.float32
        )
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        test_cases = [
            ("Hello, how are you?", "eng_Latn", "kor_Hang"),
            ("This is a test.", "eng_Latn", "kor_Hang")
        ]
        
        print("   ğŸ”„ ë²ˆì—­ í…ŒìŠ¤íŠ¸ ì¤‘...")
        for i, (text, src, tgt) in enumerate(test_cases, 1):
            result = translator(
                text, 
                src_lang=src, 
                tgt_lang=tgt,
                max_length=200,
                num_beams=2
            )
            
            print(f"   í…ŒìŠ¤íŠ¸ {i}:")
            print(f"     ì…ë ¥: {text}")
            print(f"     ì¶œë ¥: {result[0]['translation_text']}")
        
        print("âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ í•˜ì§€ë§Œ ëª¨ë¸ íŒŒì¼ì€ ì •ìƒì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return False

def update_translator_config(model_path, model_key):
    """translator.py ì„¤ì • ì—…ë°ì´íŠ¸ ì•ˆë‚´"""
    print("\nğŸ“ ì„¤ì • ê°€ì´ë“œ:")
    print("=" * 60)
    print("translator.pyì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •í•˜ì„¸ìš”:")
    print()
    print("ğŸ”§ ë°©ë²• 1: ì§ì ‘ ê²½ë¡œ ì„¤ì •")
    print("   def _find_best_model(self) -> str:")
    print(f'       return r"{os.path.abspath(model_path)}"')
    print()
    print("ğŸ”§ ë°©ë²• 2: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •")
    print(f'   export NLLB_MODEL_PATH="{os.path.abspath(model_path)}"')
    print()
    print("ğŸ”§ ë°©ë²• 3: ëª¨ë¸ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€")
    print("   preferred_models = [")
    print(f'       "{os.path.abspath(model_path)}",')
    print("       # ë‹¤ë¥¸ ê²½ë¡œë“¤...")
    print("   ]")
    print("=" * 60)
    
    # ì„±ëŠ¥ ìµœì í™” íŒ
    if model_key in ["1.3B", "3.3B"]:
        print("\nâš¡ ì„±ëŠ¥ ìµœì í™” íŒ:")
        print("1. GPU ì‚¬ìš© ê¶Œì¥ (device='cuda')")
        print("2. float16 ì‚¬ìš© (torch_dtype=torch.float16)")
        print("3. ë°°ì¹˜ í¬ê¸° ì¡°ì •")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 70)
    print("ğŸ¤– ê°œì„ ëœ NLLB ëª¨ë¸ ë‹¤ìš´ë¡œë”")
    print("=" * 70)
    
    # ì˜ì¡´ì„± í™•ì¸
    try:
        import torch
        import transformers
        print(f"âœ… PyTorch ë²„ì „: {torch.__version__}")
        print(f"âœ… Transformers ë²„ì „: {transformers.__version__}")
    except ImportError as e:
        print(f"âŒ ì˜ì¡´ì„± ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("   pip install torch transformers tqdm")
        return
    
    print()
    
    # ëª¨ë¸ ì„ íƒ
    show_model_menu()
    model_choice = get_user_choice()
    
    print(f"\nâœ… ì„ íƒëœ ëª¨ë¸: {model_choice}")
    print(f"ğŸ“¦ {AVAILABLE_MODELS[model_choice]['description']}")
    
    # í™•ì¸
    confirm = input(f"\n{AVAILABLE_MODELS[model_choice]['size']} ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    if confirm not in ['y', 'yes']:
        print("âŒ ë‹¤ìš´ë¡œë“œ ì·¨ì†Œë¨")
        return
    
    # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
    print("\n" + "=" * 70)
    model_path = download_nllb_model(model_choice)
    
    if model_path:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_success = test_model(model_path)
        
        # ì„¤ì • ê°€ì´ë“œ í‘œì‹œ
        update_translator_config(model_path, model_choice)
        
        if test_success:
            print("\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
            print("1. ìœ„ì˜ ì„¤ì • ê°€ì´ë“œë¥¼ ë”°ë¼ translator.pyë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.")
            print("2. ë²ˆì—­ í”„ë¡œê·¸ë¨ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
            print("3. ë²ˆì—­ í’ˆì§ˆ í–¥ìƒì„ í™•ì¸í•˜ì„¸ìš”!")
        else:
            print("\nâš ï¸ ëª¨ë¸ì€ ë‹¤ìš´ë¡œë“œë˜ì—ˆì§€ë§Œ í…ŒìŠ¤íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ translator.pyì—ì„œ ì§ì ‘ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
    else:
        print("\nâŒ ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
    
    print("\n" + "=" * 70)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâ¹ï¸  í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        print("ğŸ› ë²„ê·¸ ë¦¬í¬íŠ¸ë¥¼ ìœ„í•´ ìœ„ì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë³´ê³ í•´ì£¼ì„¸ìš”.")
    finally:
        input("\nê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")