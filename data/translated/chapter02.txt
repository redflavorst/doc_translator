## 인공지능 Gentlemen

여기서는 지능형 시스템의 개념, 그 환경과의 관계, 그리고 평가 방법에 대해 그리고 그러한 시스템 구축 방안에 대해 논의합니다.
## 2.1 소개

에이전트는 환경을 센서를 통해 인식하고 그 환경에 대해 반응 및 조작하는 능력을 지닌 대상이다. 인간 에이전트는 눈, 귀와 기타 감각기관으로 센서를 이용하며, 손과 입술 등 신체 부위를 통해 effectors를 사용한다. 반면 로봇 에이전트는 카메라와 적외선 레인지파인더를 센서 대체품으로, 다양한 모터를 effectors로 채택한다. 소프트웨어 에이전트는 bit 문자열로 입력된 정보를 통해 인식하고 반응한다. 일반 에이전트는 Figure 2.1에서 다이어그램화된다.

본 서적의 목표는 환경에 효과적으로 대응하는 에이전트를 설계하는 것이다. 먼저 우리가 '우수한 작업 수행'이 무엇인지 조금 더 구체적으로 정의할 계획이다. 그 다음에는 Figure 2.1의 물음표를 채우는 성공적인 에이전트 설계에 대해 논의할 것이다. 책 전반에 걸쳐 적용되는 일반적인 원칙들을 다루는데, 그 중 핵심은 에이전트가 지식을 습득해야 한다는 원칙이다. 이어서 에이전트와 환경을 연결하는 방법과 여러 유형의 환경에 대해 설명할 것이다.
## 2.2 어떻게 에이전트가 행동해야 하는가?

**이성적 에이전트**

이성적 에이전트는 올바른 일을 하는 에이전트를 의미합니다. 당연히 잘못된 일을 하는 것보다는 낫지만, 그렇다면 실제로 그 의미는 무엇일까요? 첫 번째 근사치로는 가장 성공적인 결과를 초래하는 행동을 올바른 행동으로 간주할 수 있습니다. 하지만 에이전트의 성공을 평가하는 시점과 방법에 대한 문제는 여전히 남아 있습니다.

**성능 측정**

**완전한 지식**

<!-- 이미지 -->

우리는 "성능 측정"이라는 용어를 사용하여 에이전트의 성공을 어떻게 정의할지에 대한 기준을 나타냅니다. 당연히 모든 에이전트에 대해 일률적인 한 가지 측정 기준은 존재하지 않습니다. 에이전트에게 자신의 성과에 대한 주관적인 의견을 묻는 것도 가능하지만, 일부 에이전트는 응답할 수 없으며, 다른 일부는 자기 합리화할 수 있습니다 (인간 에이전트는 특히 자기 자신을 지나치게 옹호하는 경향이 있음). 따라서 객관적이고 외부 관찰자에 의해 강제된 기준을 사용하는 것이 필요합니다. 즉, 우리는 외부 관찰자가 설정한 성공의 기준을 통해 에이전트의 환경에서 성공의 의미를 정의하고 이를 성능 측정에 적용할 것입니다.

예를 들어, 바닥 청소 에이전트가 지정된 임무를 수행하도록 지시받은 경우를 생각해 보겠습니다. 합리적인 성능 측정 방법은 한 시간 동안 청소한 먼지의 양일 수 있습니다. 더욱 정교한 측정 방법은 전력 소비량과 소음 수준도 포함할 수 있습니다. 세 번째 방법은 에이전트가 바닥을 조용하고 효율적으로 청소할 뿐 아니라 주말에는 서핑을 즐기는 시간도 가지는 경우에 고득점을 부여할 수 있습니다.

성능 평가 시점도 중요합니다. 하루 첫 시간 동안 얼마나 많은 먼지를 청소했는지 측정한다면, 초기에 빠르게 시작한 에이전트는 높은 점수를 받게 될 것이며, 꾸준히 일하는 에이전트는 그렇지 않은 경우에 비교될 것입니다. 따라서 장기적인 성과, 예를 들어 8시간 근무일이나 평생을 기준으로 측정하려 합니다.

이성적 행동과 완전한 지식 사이의 차이를 명확히 구분하는 것이 중요합니다. 완전한 지식을 가진 에이전트는 자신의 행동 결과를 완벽하게 예측하고 행동할 수 있지만, 현실에서는 불가능합니다. 예를 들어, 파리의 샹젤리제 거리를 걷다가 한 걸음 떨어져 있는 옛 친구를 보았다고 가정해 봅시다. 교통이 전혀 없고 내가 다른 일에 관심이 없다면 이성적인 에이전트는 그 친구를 가로질러 거리를 건널 것입니다. 하지만 그 순간에 비행기가 접근하게 되어 추락 사고를 겪게 된다면, 이는 이성적인 행동이었는지 여부를 의심하게 만듭니다.

성능 측정을 설정할 때 주의해야 할 사항 중 하나는 요청하는 성과 지표를 충족시키는 에이전트를 유도할 수 있다는 점입니다. 예를 들어, 성과를 먼지 청소량으로 평가한다면 아침마다 먼지가 많이 쌓인 상태에서 빠르게 청소하고 높은 점수를 받을 에이전트가 생길 수 있습니다. 실제로 원하는 것은 먼지를 얼마나 효과적으로 청소하는지 평가하는 것이지만, 그보다 복잡한 측면을 정확히 측정하는 것이 더 어렵습니다.
2 참고문헌: N. 헤딩, '보잉 747 대규모 여객기용 새 문 latch 개발', 《워싱턴 포스트》, 1989년 8월 24일.

PERCEPTUAL SEQUENCE

IDEAL rational AGENT

<!-- 이미지 -->

도로를 건너는 행위에 대한 논의: 주어진 상황에서 합리적인 에이전트는 성공 가능성이 높은 것으로 인지되었을 때 그 행동을 취해야 한다. 도로를 건너는 것 자체가 합리적이었으나, 문이 열리지 않는 상황이 발생하여 예상치 못했다. 또한, 레이더 장치가 장착된 에이전트가 철조망으로 이루어진 문을 밀어내는 등의 강력한 조치를 취하는 경우에도 성공적일 수 있으나, 이는 합리성의 수준에서 크게 다르지 않다.

즉, 에이전트가 인지하지 못한 무언가를 고려하지 못했거나 불가능한 행동을 수행하지 못한 것만으로는 합리성을 논하기 어렵다. 그러나 이러한 요건을 충족시키는 데 있어 완벽함이 요구되는 것은 공정함을 보장하기 위함이 아니라, 지능적인 에이전트가 항상 실제로 옳은 행동을 취해야 한다는 조건 하에서는 그러한 정의를 만족시키는 에이전트를 설계하기 어렵다는 점이다.

요약하면 다음과 같다:

- 성공의 기준을 정의하는 성능 지표
- 에이전트가 인지한 전체 인식 이력을 통해 파악되는 완전 인식 이력(perceptual sequence)
- 에이전트가 인식한 환경에 대한 지식
- 에이전트가 수행할 수 있는 행동들

이로부터 이상적인 합리적 에이전트의 정의는 다음과 같다: 각 가능한 인식 이력에 대해, 이상적인 합리적 에이전트는 제공된 인식 이력과 내장된 지식을 바탕으로 그 성능 지표를 극대화하는 행동을 취해야 한다.

이러한 정의를 면밀히 살펴볼 필요가 있다. 처음에는 에이전트가 명백히 무지한 행동을 허용하는 것처럼 보일 수 있다. 예를 들어,Agent가 붐비는 도로를 건널 때 양방향으로 확인하지 않고 건너는 경우, 인식 이력은 대형 트럭의 존재를 알리지 않을 수 있다. 정의는 그러한 행동을 에이전트가 허용할 수 있다고 명시할 수 있지만, 이 해석은 두 가지 측면에서 틀렸다: 첫째, 이러한 행동은 위험이 지나치게 크기 때문에 합리적이지 않다. 둘째, 이상적인 합리적 에이전트는 도로에 진입하기 전에 확인 행동을 취했을 것이며, 이는 확인이 성능을 극대화하기 때문이다. 이러한 정보 수집을 통한 유용한 정보 획득은 합리성의 핵심이며, 이는 《제16장》에서 상세히 다루어진다.
에이전트 개념은 시스템 분석 도구로 사용되며, 절대적인 특성보다는 관찰 대상을 분류하는 도구로 이해됩니다. 시계를 예로 들면, 그것은 처음에는 무생물로 간주될 수 있지만, 에이전트 관점에서는 간단한 행동을 수행하는 주체로 볼 수 있습니다. 대부분의 시계는 그 기능에 따라 시간을 조정하거나 표시하는데, 이는 그 자체의 인식 능력이 없다는 점에서 일종의 ' degenerate agent'로 볼 수 있습니다. 외부 상황에 관계없이 항상 일정한 행동을 취합니다.

그러나 이는 완전히 정확하지 않습니다. 시계와 그 소유자가 캘리포니아에서 오스트레일리아까지 여행한다면, 시계의 주요 역할은 하루를 6시간으로 재설정하는 것이 될 것입니다. 우리가 시계가 이런 실수를 했을 때 불안해하지 않는 이유는, 그들이 합리적으로 행동하고 있다는 인식 때문이며, 실제로는 다양한 지각 장비가 없기 때문입니다. 저자는 컴퓨터가 스스로 일광 절약 시간제를 설정하는 과정에서 작은 흥분을 느낀다고 합니다.

<!-- 이미지 설명은 번역되지 않음 -->
### 이상적인 인지 시퀀스에서 행동 매핑

자신이 에이전트의 행동이 현재까지의 인지 시퀀스에만 의존한다는 것을 인식할 때, 특정 인지 시퀀스에 대한 특정 행동을 기록하는 표를 이용해 어떤 에이전트를 설명할 수 있다. 대부분의 에이전트에 대해 이러한 표는 매우 길고 무한할 수 있으며, 우리가 고려하고자 하는 인지 시퀀스의 길이를 제한하지 않는 한 그러하다. 이 표는 **인지 시퀀스와 행동 간 매핑**이라 불린다. 원칙적으로, 모든 가능한 인지 시퀀스를 시도하여 에이전트의 실제 행동을 기록함으로써 어떤 매핑이 에이전트를 정확하게 묘사하는지 확인할 수 있다 (만약 에이전트가 계산 과정에서 임의화를 사용한다면, 특정 인지 시퀀스를 여러 번 실행해야 평균 행동을 추정할 수 있다). 매핑이 에이전트를 설명할 때, 이 이상적인 매핑은 이상적인 에이전트를 나타낸다. 특정 인지 시퀀스에 대한 에이전트의 행동을 명시적으로 제공하면, 이상적인 에이전트의 설계도가 마련된다.

물론, 모든 가능한 인지 시퀀스에 대한 명시적인 매핑 표를 만드는 것은 불가능하다. 매핑의 정의는 명시적인 표를 포함하거나 명시적으로 열거하지 않고도 가능하다. 매우 단순한 에이전트를 예로 들면, 계산기의 제곱근 함수가 그러하다. 이 에이전트의 인지 시퀀스는 키를 누르는 일련의 명령어들이며, 행동은 계산 결과를 화면에 표시하는 것이다. 이상적인 매핑은 입력된 양의 제곱근을 계산하고, 결과를 15자리까지 정확하게 표시하는 것이다. 이러한 이상적인 매핑의 정의는 설계자가 실제 표를 구축할 필요 없이도 가능하며, 계산기 함수의 경우 표 없이도 정확하게 작동할 수 있음을 보여준다 (그림 2.2 참조).

그림 2.2 제곱근 문제에 대한 일부 이상적인 매핑과 이를 구현하는 프로그램의 부분

| 인지 시퀀스 | 행동  |
|-------------|-------------------------------------|
|           1 | 1.000000000000000 1.048808848170152 |

**번역 출처**: Artificial Intelligence: Modern Approach I A by Stuart Russell and Peter Norvig, c 1995 Prentice-Hall, c. In AUTONOMY  
**참고**: 이미지 표시 생략
## 자율성

이상적인 이성적 주체의 정의에서 고려해야 할 또 하나의 핵심 요소는 '내장 지식' 부분이다. 에이전트의 행동이 완전히 내장 지식에 기반하여 이루어지고 주변 환경의 감각에 대해 어떠한 주의를 기울이지 않는다면, 이를 자율성이 부족하다고 판단한다. 예를 들어, 시계 제작자가 시계 소유자가 특정 국가로 이동하는 것을 예측할 수 있는 정도라면, 시계 내부에 자동 조정 메커니즘을 내장하여 시계 바늘을 매 시간 정확히 조정할 수 있는 기능을 설정할 수 있다. 이는 확실히 성공적인 행동이지만, 지능의 본질은 시계 제작자에게 있는 것이 맞다.

에이전트의 행동은 자신의 경험과 함께 환경에 적합하게 구성되는 내장 지식에 의해 기반될 수 있다. 자율성의 정의는 에이전트의 행동이 자신의 경험에 의해 결정되는 한에서 이루어진다. 그러나 완전히 초기부터의 완전한 자율성을 요구하는 것은 지나치게 엄격하다; 에이전트가 적은 경험을 갖고 있을 때는 디자이너의 도움 없이 무작위 행동을 취해야 할 수도 있다. 따라서 진화론적으로 볼 때, 동물들은 충분한 내장 능력을 갖추게 되어 스스로 생존하고 배울 수 있는 기반이 되듯이, 인공 지능 에이전트에도 초기 내장 지식과 학습 능력을 제공하는 것이 합리적이다.

자율성은 우리의 직관과도 일치하며, 훌륭한 공학적 실천 사례이다. 내장 가정을 기반으로 작동하는 에이전트는 이러한 가정들이 항상 유효해야만 성공적으로 작동하며, 따라서 유연성이 부족하다. 예를 들어, 굴속을 파는 땅벌레는 둥지를 파고 알을 낳은 후 가까운 dung heap에서 덩어리 dung을 가져와 둥지에 plugging하는 행동을 모방한다. 그러나 실제로 덩어리 dung이 손에서 빠져나가면 그 굴 속을 계속 파내고 더미 dung이 없는 것처럼 행동하지만 결코 그 사실을 인지하지 않는다. 진화는 그 벌레의 행동에 이러한 가정을 내재화시켰고, 이 가정이 위반될 경우 성공적이지 않은 행동이 결과를 낳는다. 진정한 자율성을 가진 지능형 에이전트는 다양한 환경에서 성공적으로 작동하며, 심지어 충분한 시간 동안 적응할 수 있어야 한다.
### 번역 결과:
## 2.3 스마트 에이전트 구조

에이전트 프로그램

구조

이제까지 에이전트의 동작 방식을 설명하며 주어진 감각 정보에 따른 행동을 다루어 왔습니다. 이제 에이전트 내부 작동 방식에 대해 직접 이야기해 보겠습니다. AI의 주요 임무는 에이전트 프로그램을 설계하는 것으로, 이 프로그램은 감각 정보를 입력으로 받아 행동을 출력하는 기능을 구현합니다. 이 프로그램은 특정 컴퓨팅 장치, 우리는 이를 **구조**라고 부릅니다. 당연히 선택한 프로그램은 그 기능에 부합해야 합니다.

또한 '자율적'이라는 용어는 현재 인간의 즉각적인 제어에서 벗어난 의미로 사용되며, 예를 들어 자율주행차량과 같이 해석됩니다. 그러나 우리는 이를 더 강력한 의미로 적용하고 있습니다.

인공지능: 현대 접근법 I A - 스튜어트 러셀로프와 피터 니빙스 저, 1995년 출판사: 프린트Hall  
소프트웨어 에이전트 구조  
소프트웨어 에이전트는 구조가 수용하고 실행할 수 있는 형태여야 합니다. 구조는 단순한 컴퓨터일 수 있으며, 필요에 따라 이미지 처리나 입력 필터링과 같은 특수한 하드웨어를 포함할 수도 있습니다. 또한 에이전트 프로그램과의 상호작용을 분리하는 수준의 절연 기능을 제공할 수 있습니다. 이렇게 하면 우리는 고차원적인 프로그래밍이 가능해집니다. 일반적으로 구조는 센서로부터 제공된 감각 정보를 받아들여 프로그램을 실행하고, 에이전트 프로그램이 생성한 행동 선택을 환경에 반영합니다. 에이전트, 구조, 프로그램 간의 관계는 다음과 같이 요약할 수 있습니다:
## agent = 아키텍처 + 프로그램

이 책의 대부분은 에이전트 프로그램 설계에 초점을 맞추고 있으며, 제24장과 제25장은 각각 아키텍처에 직접적으로 다루고 있습니다.

에이전트 프로그램을 설계하기 전에 다음 요소들을 잘 이해해야 합니다: 가능한 감각 경험과 행동, 달성해야 할 목표나 성능 측정 기준, 그리고 에이전트가 작동할 환경의 종류입니다. 이러한 요소들은 Figure 2.3에서 다양한 에이전트 유형 선택을 위한 기본 요소들로 제시됩니다 (5개 요소 참조).

일부 독자들은 놀라움을 느낄 수 있듯이, 우리 리스트에는 키보드 입력과 화면 출력을 기반으로 하는 완전히 인공 환경에서 작동하는 프로그램 유형들도 포함되어 있습니다. 이는 사실상의 환경이라 할 수 있는가? 라고 의문을 제기할 수 있지만, 실제로 중요한 것은 '현실적인'과 '인공적인' 환경 사이의 경계가 아닌 에이전트의 행동과 환경에서 생성되는 감각 시퀀스, 그리고 달성해야 하는 목표 사이의 복잡한 관계입니다. 일부 '현실적인' 환경들은 실제로는 매우 단순합니다. 예를 들어, 컨베이어 벨트 위를 지나가는 부품을 검사하는 로봇은 조명이 항상 적절하게 제공되고 부품만 존재하며 수용하거나 거부하는 두 가지 행동만 가능하다는 단순화된 가정을 사용할 수 있습니다.

반면에 일부 소프트웨어 에이전트는 보다 복잡하고 제한적인 환경에서 작동합니다. 예를 들어, 747 비행기 시뮬레이터를 조종하는 소프트웨어 로봇을 상상해보세요. 시뮬레이터는 매우 세밀하고 복잡한 환경이며, 소프트웨어 에이전트는 실시간으로 다양한 행동 중 하나를 선택해야 합니다. 또 다른 예로, 온라인 뉴스 소스를 스캔하고 고객에게 흥미로운 내용을 보여주는 소프트웨어 로봇을 생각해 볼 수 있습니다. 이를 잘 수행하려면 자연어 처리 능력이 필요하며, 각 고객의 관심사를 학습하고 필요에 따라 계획을 실시간으로 조정할 수 있어야 합니다.

환경에 따라 '현실적인'과 '인공적인' 구분이 모호해집니다. 예를 들어, Maes 등의 연구(1994년)에서 제시된 LIVE 환경에서는 소프트웨어 에이전트가 방 안에서 인간이 움직이는 디지털 이미지를 인식하고 행동을 선택합니다. 이 환경은 에이전트의 카메라 이미지를 대형 디스플레이 화면에 보여주고 컴퓨터 그래픽스로 에이전트를 겹쳐 표시하여 인간이 관찰할 수 있게 합니다. 한 예시로, 에이전트가 친근하게 다가오거나 손을 흔들거나 뛰어오르는 캐릭터처럼 보일 수 있습니다 (만약 인간이 요청하지 않는 한).

5. 이 용어는 간단히 **에이전트 설명(Percepts, Actions, Goals, Environment)**로 정의됩니다. 여기서 목표는 에이전트 설계 평가 기준이 되는 것이지 에이전트 내부에 반드시 반영될 필요는 없습니다.

**Figure 2.3** 에이전트 유형 예시와 그들의 Agent 설명.
| 에이전트 유형            | 감지 정보                       | 수행 작업                          | 목표                                       | 환경                     |
|------------------------|--------------------------------|-----------------------------------|-------------------------------------------|---------------------------|
| 의료 진단 시스템      | 증상, 검사 결과, 환자 응답    | 질문 제시, 진료 제시, 치료 권장  | 건강한 환자 유지, 비용 최소화               | 환자, 병원              |
| 위성 이미지 분석 시스템 | 위성 이미지의 다양한 픽셀 강도 | 이미지 분류 출력                  | 정확한 이미지 분류                        | 위성 궤도 상의 이미지    |
| 부분 선택 로봇          | 다양한 픽셀 강도               | 부품 선택 및 분류                | 부품 정확한 위치 지정                    | 컨베이어 벨트 상의 부품 |
| 정제기 제어 시스템     | 온도, 압력 값                 | 밸브 개폐, 온도 조절             | 순도 최대화, 생산량 최대화, 안전성 보장 | 정제소                    |
| 상호작용 영어 튜터    | 입력된 텍스트                   | 연습 문제 제시, 피드백 제공      | 시험 성적 향상                            | 학생 그룹               |

**가장 유명한 인공지능 환경은 튜링 테스트 환경으로, 그 핵심 목표는 현실적 존재와 인공 존재가 동등한 수준에서 경쟁하는 것이다. 그러나 이러한 환경은 매우 까다롭고 복잡하여 소프트웨어 에이전트가 동시에 인간과 같은 수준으로 수행하기가 매우 어렵다.** 섹션 2.4에서는 이러한 환경을 더욱 구체적으로 분석하며 어떤 환경이 다른 환경보다 더 요구되는지 상세히 설명하고 있다.
## 에이전트 프로그램

본 문서에서는 전반에 걸쳐 지능형 에이전트를 구축할 계획입니다. 모든 에이전트는 동일한 뼈대 구조를 갖추게 되는데, 주요 특징은 환경으로부터 감각 정보를 받아들이고 이를 바탕으로 행동을 생성하는 것입니다. 초기 버전의 에이전트 프로그램은 매우 단순한 형태를 가질 것이며 (그림 2.4 참조), 각 에이전트는 새로운 감각 정보가 들어올 때마다 업데이트되는 내부 데이터 구조를 활용합니다. 이러한 데이터 구조는 에이전트의 의사결정 과정을 통해 행동 선택을 생성하고, 그 결과는 후속 아키텍처에 의해 실행됩니다.

이 뼈대 프로그램에 대해 두 가지 중요한 점을 명심해야 합니다:

첫째, 에이전트 매핑은 감각 시퀀스를 행동으로 변환하는 함수로 정의되었으나, 에이전트 프로그램은 실제로 단일 감각 정보만을 입력으로 받습니다. 에이전트는 필요에 따라 이를 메모리에 시퀀스로 구축할 수도 있고, 일부 환경에서는 저장 없이도 성공적으로 작동할 수 있습니다. 복잡한 도메인에서는 전체 시퀀스를 저장하는 것이 실질적으로 불가능할 수 있습니다.

함수 **SKELETON -AGENT**( percep **) returns action **static **: memory**  
**UPDATE-MEMORY**( memory, percep **) action  
**CHOOSE-BEST-ACTION**( memory **) memory  
**UPDATE-MEMORY**( memory, action **) return action  

**그림 2.4**: 각 직무에서 에이전트 메모리는 새로운 감각 정보에 따라 업데이트되며, 가장 적합한 행동을 선택하고 그 행동이 취해졌음을 기록합니다. 메모리는 하나의 작업에서 다음 작업으로 지속됩니다.

둘째, 에이전트의 성능 측정 지표는 뼈대 프로그램의 일부가 아닙니다. 이는 성능 측정이 외부에서 에이전트의 행동을 평가하기 위한 것이며, 명시적인 성능 지표 없이도 고효율성을 달성하는 것이 가능하기 때문입니다 (예: 제곱근 평균 비교 참조).
## 단순히 답변을 찾아서 사용하지 않는 이유는 무엇인가?

단순히 지식 검색 테이블만을 이용하는 에이전트 프로그램을 작성하기 위한 가장 기본적인 방법은 다음과 같습니다. 이를 통해 `agent 프로그램`을 구현하는 방식은 다음과 같다:

- 메모리 내 전체 수용 가능한 시퀀스를 유지하며, 해당 시퀀스를 이용해 최적의 행동을 검색 테이블에서 찾는다.

이 제안이 실패로 귀결될 이유를 살펴보자:

- **1.** 체스와 같이 단순한 에이전트라면 검색 테이블에는 약 35,000여 개의 항목만으로 충분할 것이다.
- **2.** 테이블을 구축하는 데는 상당한 시간이 소요될 것이다.
- **3.** 에이전트는 완전히 자율적이지 않으며, 계산된 최적 행동은 내부적으로 구현되어 있어 환경 변화 시 대응 능력이 부족하다. 따라서 환경이 예상치 못한 방식으로 변할 경우 에이전트는 혼란에 빠질 수 있다.

```python
function TABLE_DRIVEN_AGENT(percept) -> action
  static 
    percepts, sequence, initially empty
    table, predefined by percept sequences, initially fully specified

  append percept to the end of action LOOKUP (percepts, table) -> return action

  percepts

  Figure 2.5: 주어진 미리 정의된 검색 테이블을 기반으로 하는 에이전트. 이 에이전트는 수용 가능한 시퀀스를 추적하고 최적의 행동만 검색한다.

  인공지능: 현대 접근법 I by Stuart Russell and Peter Norvig, 1995 Prentice-Hall

- **4.** 만약 에이전트에 학습 메커니즘을 추가하여 일정 수준의 자율성을 부여하더라도, 검색 테이블의 모든 항목에 대한 적절한 값을 학습하는 데에는 여전히 많은 시간이 필요할 것이다.

그럼에도 불구하고, `TABLE_DRIVEN_AGENT`는 우리가 원하는 기능을 수행한다: 원하는 에이전트 매핑을 구현한다. 단순히 "지능적이지 못하다"고 말하는 것만으로는 부족하며, 에이전트가 추론할 수 있도록 하고 (검색 테이블 대신), 더욱 나아가 위에서 제시한 네 가지 문제점을 피하는 것이 중요하다.
## 예시

현재 시점에서 특정 환경을 고려함으로써 논의가 더욱 구체화될 수 있습니다. 주로 이 분야의 익숙함과 그 범위가 넓어 다양한 기술적 능력을 요구하기 때문에, 자동화된 택시 운전자의 역할에 초점을 맞추어 살펴보겠습니다. 현재 기술 수준에서는 이 시스템이 다소 한계가 있으나, 대부분의 구성 요소가 활용 가능한 형태로 존재한다는 점을 독자가 당황하지 않도록 미리 지적해야 합니다. 전체 운전 작업은 매우 유연하며, 예상치 못한 상황 조합의 범위가 방대하여(이것은 논의 주제로 선택된 또 다른 이유입니다), 운전자의 상황 인식, 행동, 목표 및 환경을 먼저 고려해야 합니다. 이를 Figure 2.6에서 요약하고 이어서 논의합니다.

| 운전자 유형 | 감각 | 행동 | 목표 | 환경 |
|-------------|-------|-------|-------|-------|
| 자동택시   | 카메라, 속도계, GPS, 소나마이크로폰 | 조향, 가속, 감속, 승객과 대화 | 안전성, 속도성, 공정성, 편의성, 수익 극대화 | 도로, 교통, 보행자, 고객 |

택시 운전자는 위치 파악, 도로 상의 다른 차량 및 장애물, 속도 확인 등 필요한 정보를 제공하는 여러 감각 장치들(통제 가능한 다수의 CCTV 카메라, 속도계, 오덤터 등)로부터 얻어야 합니다. 특히 곡선 구간에서 정확한 조작이 필요하다면 가속도계가 필수적이며, 차량의 기계적 상태를 파악하기 위해 엔진 및 전기 시스템 센서도 필요합니다. GPS와 적외선 거리 감지 센서나 마이크와 키보드 같은 부가적인 장치도 고려할 수 있습니다. 결국 승객이 목적지를 알려줄 수 있도록 마이크로폰이나 키보드를 활용해야 합니다.

자동택시 운전자에게 가능한 행동은 인간 운전사와 대체로 유사하며, 주로 엔진 제어와 조향 및 제동 조작 능력이 포함됩니다. 또한 스크린 또는 음성 합성기를 통해 승객에게 피드백을 제공하거나 차량 간 통신의 가능성을 탐색할 수도 있습니다.

6 페이지 26에 기존 주행 로봇에 대한 설명이 있으며, IVHS의 지능형 차량 및 고속도로 시스템 회의 proceedings도 참고하실 수 있습니다.

조건-행동 규칙

**번역 결과 종료**
어떤 성능 측정 지표를 자동주행 차량이 지향해야 할까요? 바람직한 특성에는 다음과 같은 것들이 포함됩니다:

- 정확한 목적지 도달
- 연료 소비량 최소화 및 차량 수명 연장
- 이동 시간 및 비용 최소화
- 교통 법규 준수 및 주변 환경 disturbance 최소화
- 안전성 극대화 및 승객 편의성 증대
- 수익 극대화

이러한 목표들은 상충되는 부분이 있어 상호 간의 타협이 필요할 것입니다.

이러한 것은 실제 프로젝트로서, 차량이 직면하게 될 주행 환경을 결정하는 중요한 단계가 됩니다. 지역 도로에서 운행할 것인지, 아니면 고속도로를 활용할지 결정해야 합니다. 남부 캘리포니아와 같이 겨울이 문제가 되는 지역인지, 아니면 알래스카처럼 거의 없는 경우인지 고려해야 합니다. 영국이나 일본과 같은 지역에서도 좌회전 가능 여부에 대한 유연성을 요구할 수 있을 것입니다. 더 제한된 환경일수록 설계 과제가 커질 것입니다.

이제 실제 프로그램을 구현하기 위해 퍼셉션에서 행동으로의 매핑을 어떻게 구축할지 결정해야 합니다. 다양한 주행 스타일은 각기 다른 유형의 에이전트 프로그램을 제시할 것입니다. 고려할 에이전트 프로그램 유형은 다음과 같습니다:

- 단순 reflex 에이전트
- 환경 기록 에이전트
- 목표 기반 에이전트
- 효용 기반 에이전트
## 간단한 반응 에이전트

외부 데이터베이스 구축의 선택지는 고려할 필요가 없습니다. 한 카메라로부터 들어오는 시각 정보는 초당 50메가바이트의 속도로 전달되며 (25프레임/초, 각 프레임당 1000픽셀×8비트 색상 정보와 8비트 밝기 정보 포함), 이 룩업 테이블은 1시간 분량으로 약 2606650M 엔TRY가 필요합니다.

그러나, 테이블의 일부를 특정 빈번히 발생하는 입력/출력 연관 관계로 요약할 수 있습니다. 예를 들어, 앞 브레이크가 작동하고 그 브레이크 라이트가 켜져 있으면, 이 정보를 처리하여 특정 조건을 인식합니다 ('차량이 앞 브레이크를 사용하고 있음'). 이러한 인식은 사전에 정의된 에이전트 프로그램 내 연결을 트리거하여 특정 행동 ('경보 발생')을 실행하게 합니다. 이와 같은 연결을 '조건-액션 규칙'이라고 부르며, 다음과 같이 표기됩니다.
인간도 운전 반응과 같은 학습된 반응과 눈 주변으로 다가오는 것을 감지하는 blink 같은 선천적 반사를 포함하여 수많은 연결을 가지고 있다. 이 책에서는 그러한 연결을 학습하고 적용하는 다양한 방법들을 살펴볼 것이다.

그림 2.7은 단순한 반응 에이전트의 구조를 개념적으로 보여주며, 인식에서 행동으로 이어지는 조건-행동 관계를 어떻게 에이전트가 처리하는지 나타낸다. (이것이 사소해 보일 수 있지만, 곧 더 흥미로운 부분으로 이어진다.) 사각형은 변수를 나타낸다.

또한 상황-행동 규칙 생성 또는 IF-THEN 규칙으로도 불린다. 마지막 용어는 일부 저자들이 논리적 함의로 사용하지만, 여기서는 전부 배제한다.

그림 2.7: 단순한 반응 에이전트의 개념적 도식

```python
function SIMPLE_REFLEX_AGENT(percept) returns static: rules, action_set
state INTERPRET_INPUT(percept)
rule RULE_MATCH(state, rules)
action RULE_ACTION[rule]
return action
```

그림 2.8: 단순한 반응 에이전트. 에이전트는 현재 상황을 정의하는 인식에 부합하는 규칙을 찾은 후 해당 규칙에 따른 행동을 취한다. 현재 에이전트의 결정 과정 내부 상태를 Oval로 나타낸다. 추상화된 현재 상태 설명을 생성하는 INTERPRET_INPUT 함수와 주어진 상태 설명과 일치하는 첫 규칙을 반환하는 RULE_MATCH 함수가 보여진다. 이러한 에이전트는 매우 효율적으로 구현 가능하다 (제 10장 참조), 하지만 그 응용 범위는 제한적이다 (이후에 살펴보겠습니다).
## 세계를 추적하는 에이전트

이전에 설명한 단순형 에이전트는 현재의 지각 정보에 기반하여 올바른 결정을 내릴 수 있을 때만 작동한다. 만약 앞에 있는 차량이 최근 모델로 중앙에 설치된 제동등이 요구되는 미국과 같은 요건을 충족한다면, 단일 이미지로 제동 중인지 판단할 수 있다. 그러나 더 오래된 모델들은 제동등의 구성이 다르기 때문에 이러한 판단이 항상 가능하지 않다. 따라서 간단한 제동 규칙에서도 운전자가 특정 상태를 유지해야 하며, 이 상태는 단순히 이전 프레임의 카메라 이미지에서 차량의 양쪽 가장자리에 제동등이 켜지는지 확인하는 것만으로는 부족하다. 즉, 에이전트는 가시적인 상태를 통해 이전 프레임의 정보를 결합하여 현재 상태를 업데이트해야 한다.

### 내부 상태

**목표**

**탐색**

**계획**

- 제동등, 방향등, 직진 신호등 등이 있지만, 이들로는 항상 차량의 제동 여부를 명확히 알 수 없다.
- 따라서 간단한 제동 규칙에서도 운전자는 에이전트가 어떤 행동을 취해야 하는지 결정하기 위해 일부 형태의 내부 상태를 유지해야 한다. 여기서 내부 상태는 과도하게 복잡하지 않으며, 단지 카메라 이미지에서 이전 프레임과 비교하여 차량의 양쪽 가장자리에 제동등이 켜지기 시작하는 시점을 감지하는 데 필요하다.

### 예시 사례

정기적으로 운전자는 후방 시야 거울을 통해 주변 차량의 위치를 확인한다. 만약 운전자가 시야에서 벗어난다면, 다음 차선의 차량들이 가시 상태에 있다. 운전자가 거울을 통해 차량을 확인하지 않는다면, 거울에 나타난 차량들은 이미 사라진 상태이다. 따라서 차선 변경을 시도할 때, 차량들이 실제로 존재하는지 여부를 알기 위해 내부 상태의 정보가 필요하다.

### 문제점

이러한 예시가 보여주는 문제는 센서가 세계의 전체 상태에 대한 접근을 제공하지 않기 때문에 발생한다. 이러한 경우 에이전트는 특정 지각 입력을 생성하지만 서로 다른 행동에 따라 의미 있는 차이가 발생하는 세계 상태들 사이를 구분하기 위해 내부 상태 정보를 유지해야 한다. 여기서 '의미 있는 차이'는 두 상태에 따라 다른 적절한 행동이 요구됨을 의미한다.

### 내부 상태 업데이트

내부 상태 정보의 업데이트는 에이전트 프로그램에 다음 두 가지 유형의 지식이 필요하다:

1. **세계의 독립적 진화에 대한 지식**: 에이전트 외부 요인에 의한 세계의 변화를 이해하는 것, 예를 들어, 추월 차량이 일반적으로 이전 위치보다 가까워진다.
2. **에이전트 행동의 영향에 대한 지식**: 에이전트의 행동이 세계 상태에 미치는 영향을 이해하는 것, 예를 들어, 오른쪽 차선으로 변경 시 일시적으로 해당 차선에 공백이 생기는 경우.

### Figure 참조

Figure 2.9는 단순형 에이전트의 구조를 보여주며, 현재 지각 정보와 이전 내부 상태의 조합으로 현재 상태에 대한 업데이트된 설명을 생성한다. Figure 2.10은 에이전트 프로그램의 구조를 나타낸다. 특히, `상태-가치 함수`가 주목할 만한데, 이는 새로운 내부 상태 설명을 생성하고 기존 지식을 활용하여 지각 정보를 해석하는 역할을 한다. 또한 에이전트의 행동이 세계 상태에 미치는 영향을 추적해야 함을 강조한다. 자세한 예는 Chapters 7과 17에 나와 있다.
## 목표 기반 에이전트

현재 환경 상태만으로는 결정해야 할 행동을 명확히 선택하기 어렵습니다. 예를 들어, 교차로에서 택시 운전사가 직진, 좌회전, 우회전 중 어떤 행동을 취해야 할지는 현재 위치뿐만 아니라 목적지에 대한 정보 또한 중요합니다. 목표 기반 에이전트는 현재 상태뿐만 아니라 목표 상황을 기술하는 정보가 필요합니다. 예를 들어, 승객의 목적지에 도달하는 것이 바람직한 상황입니다. 에이전트 프로그램은 현재 상태 정보와 가능한 여러 행동 결과에 대한 정보를 결합하여 목표 달성에 유리한 행동을 선택합니다. 때로는 한 가지 행동으로 목표가 즉시 달성되기도 하지만, 때로는 다양한 경로의 우회와 같은 복잡한 과정을 고려해야 할 수도 있습니다. 인공지능의 하위 분야인 탐색(챕터 3~5)과 계획(챕터 11~13)은 이러한 목표 달성을 위한 행동 시퀀스를 찾는 데 중점을 둡니다.

**반영 에이전트 내부 상태**

함수 REFLEX_AGENT_WITH_STATE(percept)는 다음을 반환합니다:
- **상태**: 현재 세계 상태와 규칙 집합에 대한 설명
- **action_static**: 규칙 집합과 조건-행동 매핑
- **state_UPDATE**: 상태 업데이트 함수 (기존 상태, 지각)
- **RULE_MATCH**: 현재 상태와 일치하는 규칙 찾기 함수 (상태, 규칙 집합)
- **RULE_ACTION**: 해당 규칙의 행동 실행 함수 (상태, 규칙)

**반영 에이전트 내부 상태 설명**

반영 에이전트는 내부 상태를 통해 현재 상황과 기존 규칙을 비교하여 적합한 행동을 결정합니다. 구체적으로, 지각 정보와 저장된 내부 상태를 결합하여 현재 상황에 가장 적합한 규칙을 찾고 그에 따른 행동을 수행합니다. 주의할 점은 이러한 방식의 결정 과정은 미래 지향적 고려사항을 포함하는 반면, 앞서 설명된 조건-행동 규칙 방식은 이러한 측면을 명시적으로 다루지 않아 미리 계산된 최적 행동을 따릅니다. 반영 에이전트는 브레이크 램프를 만났을 때 즉시 정지합니다. 반면 목표 기반 에이전트는 도로 상황이 정상일 때 차량 충돌을 피하는 것이 목표임을 인지하고, 앞 차량이 브레이크 램프를 켜지 않았다면 자연스럽게 정지할 것입니다. 따라서 미래 상황에 대한 유연한 대응이 가능합니다. 만약 반영 에이전트라면 새로운 목적지에 도착하기 위해 좌회전이나 직진 규칙을 모두 수정해야 하지만, 목표 기반 에이전트는 새로운 목적지에 대한 행동을 쉽게 수정할 수 있습니다.
그림 2.11은 목표 기반 에이전트의 구조를 보여준다. 이 장에서는 목표 기반 에이전트를 위한 상세한 GAs 프로그램들이 포함되어 있다.

 에이전트는 명시적인 목표를 갖고 있다.
## 효용 기반의 원칙

목표 자체만으로는 고도로 질 높은 행동을 생성하는 데 충분하지 않다. 예를 들어, 목적지에 도착하는 특정 행동 시퀀스들이 있어 목표를 달성할 수 있지만, 더욱 빠르고 안전하며 신뢰할 수 있는 옵션들이 존재한다. 이러한 경우, 목표는 단지 '행복'과 '불행' 상태 사이의 단순한 구분을 제공하며, 더 일반적인 성능 지표로 다양한 세계 상태 또는 상태들의 시퀀스 간의 비교를 가능하게 하지 못한다. 즉, 어떤 상태가 더 '행복'한지를 정확히 정의하기보다는, 에이전트가 특정 상태를 선호하게 되는 경향을 나타내는 '유용도' 개념이 사용된다. '행복'이라는 용어가 과학적으로 적절하지 않기 때문에, 선호하는 상태가 더 높은 유용도로 해석되어야 한다.

유용도는 상태와 실수 값 간의 매핑을 통해 작동하며, 여기에는 에이전트가 경험하는 '행복'의 정도가 포함된다. 구체적으로, 유용 함수의 완전한 정의는 다음과 같다:

1. **충돌하는 목표**: 만약 일부 목표만을 실현 가능한 경우(예: 속도와 안전성), 유용 함수는 이러한 목표 간의 적절한 우선순위를 명시한다.
2. **다수의 목표**: 여러 목표를 추구할 수 있으나, 어떤 목표들은 달성 불가능할 수 있다. 이때 유용 함수는 특정 행동 방식의 성공 가능성에 대한 가중치를 제공한다.

'유용도'는 '유용성'을 의미하며, 전기 회사나 수도 시설과는 관련이 없다. 여기서 '장기간의 상태 시퀀스'를 측정할 때 유용도를 사용한다. 

결정적인 선택을 내리는 합리적인 에이전트는 유용도 함수를 가지고 있으며, 이를 통해 에이전트는 성공 가능성과 목표의 중요성을 비교하여 결정을 내린다.

제16장에서는 합리적인 에이전트가 유용도 함수를 가지고 있음을 보여준다. 구체적으로 표현된 유용도 함수를 가진 에이전트는 즉시 목표를 만족시키는 행동을 선택할 수 있으나, 다른 행동 옵션들 간의 유용도를 비교해야 할 경우도 있다. 

이러한 효용 기반 구조는 제2장 제12절의 그림 2.12에서 확인 가능하며, 제5장에서 게임 플레이 프로그램에서 상세하게 다루어진다. 또한 제17장에서는 일반적인 의사결정 에이전트 설계 문제를 다룬다.

**Figure 2.12: 완전한 효용 기반 구조**

<!-- 이미지 -->
## 환경 개요

본 섹션 및 장의 연습 문제에서는 특정 에이전트를 환경에 연결하는 방법을 보여줍니다. 이전 섹션 2.3에서는 여러 종류의 에이전트와 환경에 대해 소개하였습니다. 그럼에도 불구하고, 이들 간의 연결 방식은 본질적으로 동일합니다: 에이전트가 환경에 행동을 수행하고, 그 결과 환경으로부터 인식 정보를 제공받습니다.

### 환경의 다양한 유형

먼저, 다양한 유형의 환경과 그들이 에이전트 설계에 미치는 영향에 대해 설명하겠습니다. 그 다음으로는 테스트 플랫폼으로 활용 가능한 환경 프로그램 및 에이전트 프로그램에 대해 설명하겠습니다.

### 환경 특성

환경은 여러 가지 맛으로 나타납니다. 주요 구분 기준은 다음과 같습니다:

#### 접근성 vs. 비접근성

만약 에이전트의 감각 장치가 환경 전체 상태에 대한 완전한 접근성을 제공한다면, 그 환경은 에이전트에게 접근 가능하다고 말합니다. 이와 같이 효과적으로 접근 가능한 환경에서는 에이전트의 센서가 선택에 필요한 모든 관련 측면을 감지하므로 에이전트는 내부 상태를 유지할 필요 없이 세상을 추적할 수 있습니다.
## 결정론 vs 비결정론

환경의 다음 상태가 현재 상태와 에이전트가 선택한 행동에 의해 완전히 결정된다면, 우리는 그 환경을 **결정론적** 원리로 말할 수 있다. 이때 에이전트는 불확실성에 대해 걱정할 필요가 없다. 만약 환경이 접근 가능하다면, 그 자체로 비결정적일 수 있지만 관찰하기 어려운 요소들이 존재할 수 있다. 따라서 특히 복잡한 환경에서 모든 측면을 추적하는 것은 어려울 수 있으므로, 에이전트의 시점에서 환경을 결정론적이거나 비결정론적으로 사고하는 것이 종종 더 유용하다.

## 에피소드 vs 비에피소드적 환경

에피소드적 환경에서 에이전트의 경험은 개별적인 에피소드로 나뉜다. 각 에피소드는 에이전트가 인식하고 행동하는 과정으로 구성되며, 그 에피소드 내에서의 행동의 효과는 그 자체에만 의존하고 이후의 에피소드에 영향을 주지 않는다. 따라서 에이전트는 미래 에피소드에 대한 예측 없이 행동할 수 있어 복잡성이 덜하다.

## 정적 vs 동적 환경

환경이 에이전트가 의사결정 과정 중에 변화할 수 있다면, 그 환경은 해당 에이전트에 대해 **동적**으로 간주된다. 반면에 환경이 변화하지 않는다면 정적으로 분류된다. 정적 환경에서는 에이전트가 의사결정 과정에서 세계를 지속적으로 관찰할 필요가 없고 시간의 흐름에 대한 걱정도 없다. 만약 환경 자체는 변하지 않지만 에이전트의 성과가 시간에 따라 평가된다면, 그 환경은 **부분적으로 동적**이라고 말할 수 있다.
### 이산성 vs 연속성

만약 고유하고 미리 정의된 여러 감각 경험과 행동이 제한적이라면, 환경은 이산적이라 할 수 있습니다. 이 경우, 각 순간에 가능한 개체 수 C는 고정되어 있습니다. 반면에, 택시 운전 환경은 연속적입니다 - 각 택시와 다른 차량의 속도와 위치는 연속적인 값의 범위를 통해 움직입니다.

다양한 환경 유형은 각각에 효과적인 에이전트 프로그램의 개발을 요구합니다. 실제로는 가장 어려운 상황이 비순환적이고 비예측 가능하며 연속적이라는 점을 예상하시다시피 입니다. 이러한 이유로 실제 상황들은 매우 복잡하여 예측 가능성 여부는 논란의 대상이 되지만, 실질적인 목적으로는 비결정론적 처리가 필요합니다.

10개의 수준으로 미세하게 세분화된 환경에서도 택시 운전 환경은 이산적이라 볼 수 있습니다. 왜냐하면 카메라 이미지가 이산적인 픽셀 값으로 디지털화되기 때문입니다. 그러나 합리적인 감각 프로그램은 이러한 수준을 넘어서 연속적인 수준까지 추상화해야 합니다.

**Figure 2.13**은 친숙한 여러 환경의 특성을 나열합니다. 환경과 에이전트를 어떻게 개념화하느냐에 따라 답변이 달라질 수 있음을 주목하십시오. 예를 들어, 카드의 순서를 추적할 수 있는 경우 포커는 결정론적이지만 그렇지 않은 경우 비결정론적입니다. 또한 많은 환경은 높은 수준에서 에이전트 개별 행동을 벗어난 연속적인 에피소드로 구성되어 있습니다. 예를 들어, 체스 대회는 일련의 게임으로 이루어져 있어 각 게임은 독립적인 에피소드입니다. 따라서 한 게임 내 개체들 간의 상호작용이 전체 에이전트 성과에 영향을 미치지 않습니다. 반면에 여러 게임 내 개체들 간의 상호작용은 에이전트가 여러 게임을 미리 고려해야 하기 때문에 연속적인 수준에서 고려해야 합니다.

| 환경                                                                                                                                                   | 접근 가능                 | 결정론적             | 순환적                | 정지성                      | 이산적                           |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------|-----------------------|-----------------------|-----------------------------|----------------------------------|
| 시계 없는 체스 체스 시계 없는 포커 백게임 택시 운전 의료 진단 시스템 이미지 분석 시스템 부분 작업 로봇 제조 컨트롤러 인터랙티브 영어 튜터 | 예                      | 예                    | 없음                 | 준 정지                      | 예                               |
|                                                                                                                                                          |                               | 없음                  | 없음                 |                            | 예                                |
|                                                                                                                                                          |                           | 없음                  | 예                     |                           | 예                                |
|                                                                                                                                                          |                           | 없음                  | 없음                 | 준 정지                      | 예                                |
|                                                                                                                                                          |                           | 없음                  | 없음                 |                            | 예                                |
|                                                                                                                                                          |                           | 예                    | 없음                 |                            | 예                                |
|                                                                                                                                                          |                           | 예                    | 없음                 |                            | 예                                |
|                                                                                                                                                          |                           | 예                    | 없음                 |                            | 예                                |
|                                                                                                                                                          |                           | 예                    | 예                     |                            | 예                                |
|                                                                                                                                                          |                           |                     | 없음                 |                            | 예                                |
|                                                                                                                                                          |                           |                     | 없음                 |                            | 예                                |
|                                                                                                                                                          |                           |                     |                     |                            | 예                                |

**Figure 2.13**

환경과 그 특성의 예시 목록.
### 일반 환경 프로그램

일반 환경 프로그램은 Figure 2.14에서 기본적인 에이전트와 환경 간의 관계를 간략하게 도식화한다. 이 책에서는 예시와 연습문제들을 통해 환경 시뮬레이터를 활용함으로써 편리함을 느낄 수 있다. 시뮬레이터는 입력으로 에이전트들을 받아들여 각 에이전트가 환경에서 관찰 가능한 감각 정보를 반복적으로 수집하고 이를 기반으로 행동을 취한다. 그런 다음 시뮬레이터는 이러한 행동에 따라 환경을 업데이트하고, 에이전트가 고려하지 않은 환경 내 동적 과정 (예: 물리적 현상 등)도 처리할 수 있다. 따라서 환경은 초기 상태와 업데이트 함수에 의해 정의된다. 물론, 시뮬레이터 환경에서 작동하는 에이전트는 실제 환경에서도 동일한 종류의 감각 정보를 제공받고 같은 종류의 행동을 수용할 것이다.

RUN -ENVIRONMENT 절차는 에이전트들을 적절히 테스트한다. 자연어 대화에 참여하는 에이전트와 같은 일부 에이전트들의 경우에는 그들의 행동만 관찰하는 것으로도 충분하다. 에이전트 성능에 대한 더 자세한 정보를 얻기 위해서는 에이전트 성능 측정 코드를 삽입할 수 있다. 함수 RUN -EVAL -ENVIRONMENT는 Figure 2.15에 표시된 바와 같이 각 에이전트에 대해 성능 지표를 적용하고 결과 리스트를 반환하며, 각 에이전트의 성능 점수를 추적한다.

일반적으로 성능 지표는 프로그램 실행 중 생성된 환경 상태의 전체 시퀀스에 따라 달라질 수 있지만, 보통은 특정 환경 조건에 종속적이다.

**인공지능: 현대 접근법 I**  
스타우트 러셀과 피터 노르비그 저, 1995년, 프리엔치-해리올  
**프로그램 설명**

```plaintext
procedure RUN -ENVIRONMENT(state, UPDATE-FUNCTION inputs : state,
                                 agents : set of agents,
                                 termination : predicate )
begin
    repeat
        for each agent in agents do
            PERCEPT[agent] = GET-PERCEPT(agentstate, );
        for each agent in agents do
            ACTION[agent] = PROGRAM[agent](PERCEPT[agent]);
        state UPDATE-FUNCTION(actions, agents, state);
    until termination(state);
end;
```

**성능 평가**

```plaintext
function RUN -EVAL -ENVIRONMENT(state, UPDATE-FUNCTION, agents, termination, PERFORMANCE-FN) returns scores local variables : scores, vector same size as agents, all 0
repeat
    for each agent in agents do
        PERCEPT[agent] = GET-PERCEPT(agentstate, );
    for each agent in agents do
        ACTION[agent] = PROGRAM[agent](PERCEPT[agent]);
    state UPDATE-FUNCTION(actions, agents, state);
    scores = PERFORMANCE-FN(scores, agents, state);
until termination(state);
return scores /* 변경 */
```

**환경 시뮬레이터**

Figure 2.15: 각 에이전트별 성능 추적을 위한 환경 시뮬레이터 프로그램.
작업은 단순히 누적(accumulation), 평균(averaging), 또는 최대값(maximum)을 사용하여 수행됩니다. 예를 들어, 진공청소기 에이전트의 성능 측정이 한 시간 동안 제거된 먼지의 총량이라면, 사용자는 현재까지 제거된 먼지의 양을 추적할 것입니다.

RUN-EVAL -ENVIRONMENT 모드는 특정 날짜 함수로 정의된 단일 환경에서의 성능 측정값을 반환합니다. 일반적으로 에이전트는 초기 상태에서 목표 상태로 전환하는 데 설계됩니다.

Artificial Intelligence: Modern Approach I by Stuart Russell and Peter Norvig, 1995. Prentice-Hall.
## 환경 클래스

환경 클래스에서는 다양한 환경 집합을 활용하여 작업합니다. 예를 들어, 여러 종류의 인간과 기계 간의 상대방이 포함된 광범위한 컬렉션의 체스 프로그램을 설계합니다. 단일 상대에게 설계된 경우 특정 약점을 악용할 수 있지만, 이는 일반적인 플레이에 적합한 프로그램을 제공하지 못할 것입니다. 따라서 에이전트의 성능을 정확히 측정하기 위해서는 특정 환경을 선택하여 실행하는 환경 생성기가 필요합니다. 우리는 관심 있는 에이전트의 환경 클래스 내 평균 성능을 측정합니다. 이는 시뮬레이션 환경에서 비교적 간단하게 구현될 수 있으며, Exercises 2.5부터 2.11까지 해당 개발 과정과 관련된 측정 방법을 안내합니다.

상태 변수 간 혼동이 발생할 수 있습니다: 환경 시뮬레이터 내 상태 변수와 에이전트 자신의 상태 변수 사이입니다 (AGENT -WITH-STATE). 프로그래머로서 환경 시뮬레이터와 에이전트를 모두 구현할 때 에이전트에게 환경 시뮬레이터의 상태 변수를 살펴보도록 허용하는 유혹이 생깁니다. 이러한 유혹은 반드시 경계해야 합니다. 에이전트 버전의 상태는 에이전트의 감각 정보만으로 구성되어야 하며, 가장 완전한 상태 정보에 대한 접근은 배제되어야 합니다.
이 장의 핵심 요약은 다음과 같습니다:

- 에이전트는 환경에서 인식하고 행동하는 존재로, 설계자에 의해 구조화된 프로그램의 형태로 나누어집니다.
- 이상적인 에이전트는 현재까지 인식한 퍼셉트 시퀀스를 바탕으로 성능 지표를 최대화하는 행동을 항상 취합니다.
- 에이전트는 자신의 경험에 따라 자율적으로 행동하며, 설계자가 미리 구축한 환경 지식에 의존하지 않습니다.
- 에이전트 프로그램은 퍼셉트에서 직접 행동으로 맵핑하며, 내부 상태를 업데이트합니다.
- 다양한 기본 에이전트 프로그램 설계가 존재하는데, 정보의 명시성 및 활용 방식에 따라 효율성, 간결성, 처리 속도 및 유연성이 달라집니다. 적절한 에이전트 프로그램 설계는 인식, 행동, 목표, 환경에 근거합니다.
- 반사 에이전트는 즉시 반응하며, 목표 기반 에이전트는 설정된 목표를 달성하려고 하며, 유틸리티 기반 에이전트는 개인적인 '행복' 극대화를 목표로 합니다.
- 지식을 논리적으로 추론하여 결정을 내리는 과정은 AI의 핵심이며, 성공적인 에이전트 설계의 중심입니다. 따라서 지식 표현이 중요합니다.
- 일부 환경은 다른 환경보다 더욱 복잡하고 접근하기 어렵고 예측 불가능하며 반복적이지 않고 연속적입니다. 이러한 환경은 가장 도전적입니다.
## 참고문헌 및 역사적 논문

인간의 인지 과정에서 Perception 시퀀스를 행동으로 매핑하는 분석은 주로 행동주의 이론에서 비롯되며, 특히 Skinner(1953)와 같은 행동주의자들의 접근법을 통해 생물학적 행동을 엄격한 입력/출력 시스템에서 자극/반응 시스템으로 재구성하려는 노력에서 비롯되었다. 컴퓨터 메타포를 활용한 인지 심리학의 발전은 기능주의적 접근법을 도입함으로써 에이전트의 내부 상태를 그림에 반영하게 되었다 (Putnam, 1960; Lewis, 1966). 철학자 Daniel Dennett(1969; 1978b)는 이러한 관점들을 통합하여 에이전트의 의도적 자세를 구축하였다. 한편으로는 AI 분야에서도 높은 수준의 추상적 관점이 제시되었는데, 특히 McCarthy와 Hayes(1969)의 접근법이 그러하다. Jon Doyle (1983)은 합리성 기반 설계가 AI의 핵심이며, 이는 AI의 사명으로 남아 다른 주제들이 새로운 학문 분야로 발전할 수 있음을 제안하였다. 합리성은 자원 제약, 절차적 특성에 기반한 합리성과 같이, 합리적 선택을 객관적 합리성으로 정의한다 (Simon, 1958). Cherniak(1986)은 합리성의 최소 단위가 에이전트의 정의에 이르기까지 이어진다는 점을 탐구하였고, Russell과 Weil(1991)은 다양한 에이전트 아키텍처를 활용한 명시적 논의를 진행하였다. Dung의 개미 집단 생태학 연구(Hanski와 Cambefort, 1991)는 개미의 행동에 대한 풍부한 통찰을 제공한다.

- 2.1 성능 측정치와 유틸리티 함수의 차이점은 무엇인가?
- 2.2 Figure 2.3의 각 환경에 대해 가장 적합한 에이전트 아키텍처 (전략적 표제, 단순 반사, 목표 지향적 또는 유틸리티 지향적)를 결정하라.
- 2.3 익숙한 도메인을 선택하고 해당 환경에 대한 에이전트의 설명과 작성을 수행하라. 환경을 접근성이 있으며 결정론적이고 순환적이지 않거나 정적이고 연속적이지 않다고 특징지어라. 어떤 에이전트 아키텍처가 가장 적합한가?
- 2.4 운전 중에 최선의 정책은 무엇인가?
  - 항상 방향 지시기를 먼저 켜고 운전하라.
  - 방향 지시기를 사용하지 말라.
  - 거울을 확인하고, 앞에 보이는 차가 있는지 확인할 때만 방향 지시기를 사용하라.

이 정책을 달성하기 위해 필요한 추론 유형은 무엇인가? (논리적, 목표 지향적, 유틸리티 지향적) 필요한 에이전트 설계 유형은 무엇인가? (반사적, 목표 지향적, 유틸리티 지향적)
## 환경 및 에이전트 구현 exercises

---

### 환경 및 에이전트 시뮬레이션 구현

이 exercises는 진공청소기 세계에서 환경과 에이전트의 구현에 초점을 맞춥니다.

#### 센서 정보

각 진공청소기 에이전트는 매 턴마다 세 개의 센서 정보 벡터를 받습니다.
- **첫 번째 요소**: 터치 센서로, 기계가 무언가에 부딪히면 1, 그렇지 않으면 0을 반환합니다.
- **두 번째 요소**: 머신 내장 카메라 센서로, 장애물이 존재하면 1, 그렇지 않으면 0을 반환합니다.
- **세 번째 요소**: 적외선 센서로, 에이전트가 집 위치에 있으면 1, 그렇지 않으면 0을 반환합니다.

#### 가능한 행동

다음 다섯 가지 행동이 가능합니다:
1. 앞으로 이동
2. 오른쪽으로 90도 회전
3. 왼쪽으로 90도 회전
4. 먼지 흡입
5. 정지 (turn off)

#### 목표

각 에이전트의 목표는 청소하고 집으로 돌아가는 것입니다. 구체적으로는 다음과 같습니다:
- 매 먼지 조각을 청소할 때마다 100점 획득
- 각 행동 시마다 -1점 차감
- 집으로 돌아오지 못하고 집 위치에 도착하지 못한 경우 -1000점 차감

#### 환경 설정

환경은 격자형 구조로 구성됩니다. 격자 내에는 장애물(벽, 가구 등), 비어있는 공간이 공존하며 일부 공간에는 먼지가 존재합니다. '앞으로 이동하기' 행동은 장애물 없이 이동할 수 있으며, 장애물이 있는 공간에서는 해당 위치에서 멈추고 터치 센서가 활성화됩니다. '먼지 흡입' 행동은 먼지를 제거하고, '정지' 명령은 시뮬레이션 종료를 의미합니다.

환경 복잡도는 세 가지 차원으로 조절 가능합니다:
- 방의 형태: 가장 단순한 경우에는 방이 정사각형으로 고정된 크기 \( n \times n \)이며, 여기서 \( n \)은 미리 정해진다. 방 형태를 직사각형이나 불규칙형으로 변경하거나 연결된 여러 방(길이가 있는 경로를 통해 연결된 방들의 시리즈)으로 만들어 복잡성을 높일 수 있다.

- 가구 배치: 한 개의 방에 가구를 배치하면 빈 방보다 더 복잡해진다. 진공청소기와 같은 청소 도구를 사용할 때, 가구와 벽 사이의 구분이 시각적으로 쉽게 이루어지지 않으며, 터치 센서에서도 동일하게 보인다.

- 먼지 배치: 가장 단순한 상황에서는 먼지가 방 전체에 균일하게 분포된다. 하지만 실제 상황에서는 특정 위치에 먼지가 더 많이 집중되어 방의 앞쪽이나 다음 방으로 이어지는 경로에 따라 분포한다.

- 2.6 테이블 기반 검색 구조 구현: 진공청소기 세계는 \( 2 \times 2 \) 크기의 오픈 정사각형 그리드 형태로 구성되며, 최대 두 개의 정사각형에만 먼지가 분포되어 있다. 에이전트는 상왼쪽 모서리에서 오른쪽을 향하며 시작하고, 각 턴마다 여덟 가지 가능한 퍼셉트 벡터 중 하나를 취한다. 테이블의 크기는 \( 9 \times 9 \)이며 총 \( 18 \times 9^{i} = 153,391,688 \) 가지 경우가 존재하나, 터치 센서와 홈 센서 입력은 불필요하게 여겨져 에이전트가 벽에 부딪히지 않고 시작 지점으로 돌아올 수 있도록 배열하여 필요 없는 벡터를 줄일 수 있다. 따라서 에이전트는 최대 두 개의 벡터만 고려하면 되므로 테이블 크기는 최대 \( 9 \times 2 = 18 \)까지 줄어들어 \( 2^{1} \times 2^{10} = 2^{12} = 4096 \)만 된다. 여러 가능한 세계를 시뮬레이션하고 각 세계의 성능 점수와 평균 점수를 기록한다.

- 2.7 직사각형 방 환경 구현: 길이 \( a \)와 너비 \( m \)가 무작위로 선택된 \( 8 \leq a, m \leq 15 \) 범위 내의 직사각형 방 환경을 구현한다.

- 2.8Exercise 2.7 환경의 완전 참조 에이전트 설계 및 구현: 복귀 요구사항을 무시하고 참조 에이전트의 성능을 측정한다. 참조 에이전트가 항상 종료하고 다시 돌아오는 것이 불가능함을 설명하고, 최상의 참조 에이전트의 역할은 무엇인지 탐색한다. 참조 에이전트가 잘 작동하지 못하는 원인을 논의한다.

- 2.9 내부 상태 기반 여러 에이전트 설계 및 구현: 내부 상태 기반 여러 에이전트의 성능을 측정하고, 이상적인 에이전트와의 유사성을 분석한다.

- 2.10Exercise 2.7 도메인에서 테이블 기반 검색 구조의 테이블 크기 계산: \( 2 \times 2 \) 그리드에서의 테이블 크기를 계산하며, 구체적인 계산 과정을 설명한다.

- 2.11 환경 변화 및 먼지 분포 실험: 방 형태와 먼지 분포를 변경하고 가구를 추가한 새로운 환경에서 에이전트들의 성능을 측정한다. 이를 통해 더 복잡한 지형에 대한 성능 향상 방안을 논의한다.